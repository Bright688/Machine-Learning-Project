{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf5fbfc-2d55-4bd4-8ab3-5c5cb635991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running web url input and Feature Selection\n"
     ]
    }
   ],
   "source": [
    "#1-2. Web URL input and Feature Selection\n",
    "print(\"Running web url input and Feature Selection\")\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import whois\n",
    "from datetime import datetime\n",
    "import socket\n",
    "import ssl\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "from urllib3.exceptions import NewConnectionError, MaxRetryError\n",
    "from requests.exceptions import ConnectionError\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Define headers to mimic a real browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Referer': 'https://google.com',  # Optional\n",
    "\n",
    "}\n",
    "\n",
    "# Fetch URL with retries\n",
    "def fetch_url(url, retries=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx, 5xx)\n",
    "            return response.content\n",
    "        except (requests.exceptions.RequestException, NewConnectionError, MaxRetryError, ConnectionError):\n",
    "            # Suppress error details and retry\n",
    "            time.sleep(2)  # Delay between retries\n",
    "    # If all retries fail, return None\n",
    "    return None\n",
    "\n",
    "                \n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "                                             #Domain-based Feature\n",
    "# Function to check SSL certificate\n",
    "def check_ssl(domain):\n",
    "    try:\n",
    "        context = ssl.create_default_context()\n",
    "        with socket.create_connection((domain, 443)) as sock:\n",
    "            with context.wrap_socket(sock, server_hostname=domain) as secure_sock:\n",
    "                return True\n",
    "    except:\n",
    "        return False\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "                                           \n",
    "\n",
    "                                           \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "                                               #URL-based Feature\n",
    "# Check for IP address in URL\n",
    "def contains_ip(url):\n",
    "    ip_pattern = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "    return bool(ip_pattern.search(url))\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "                                              #URL-based feature\n",
    "# Compile the shortening services regex pattern\n",
    "shortening_services_pattern = re.compile(r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\"\n",
    "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\"\n",
    "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\"\n",
    "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\"\n",
    "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\"\n",
    "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\"\n",
    "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\"\n",
    "                      r\"tr\\.im|link\\.zip\\.net\")\n",
    "\n",
    "# Count special characters in URL\n",
    "def count_special_chars(url):\n",
    "    return int(sum(not c.isalnum() and c not in ['.', '-', '_', ':', '/', '?', '&', '=', '%'] for c in url))\n",
    "\n",
    "# Check for URL shortening services using the provided regex pattern\n",
    "def shortening_services(url):\n",
    "    return bool(re.search(shortening_services_pattern, url))\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Extract features from URL\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "    domain = urlparse(url).netloc\n",
    "    #-----------------------------------------------------------------------------------------------------------------\n",
    "                                              #URL-based features\n",
    "    features['url_length'] = int(len(url))\n",
    "    features['contains_ip'] = int(contains_ip(url))\n",
    "    features['shortening_services'] = int(shortening_services(url))\n",
    "    features['special_chars'] = int(count_special_chars(url))\n",
    "    #-----------------------------------------------------------------------------------------------------------------\n",
    "    content = fetch_url(url)\n",
    "    if content:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        #-------------------------------------------------------------------------------------------------------------\n",
    "                                              #Content-based features\n",
    "        features['html_length'] = int(len(content))\n",
    "        features['js_length'] = sum(len(s.string) for s in soup.find_all('script') if s.string)\n",
    "        features['num_links'] = len(soup.find_all('a'))\n",
    "        features['num_forms'] = len(soup.find_all('form'))\n",
    "        \n",
    "       \n",
    "        #-------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        try:\n",
    "            #---------------------------------------------------------------------------------------------------------\n",
    "                                              #Domain-based features\n",
    "            domain_info = whois.whois(domain)\n",
    "            creation_date = domain_info.creation_date\n",
    "            updated_date = domain_info.updated_date\n",
    "            expiration_date = domain_info.expiration_date\n",
    "            \n",
    "            if isinstance(creation_date, list):\n",
    "                creation_date = creation_date[0]\n",
    "            if isinstance(updated_date, list):\n",
    "                updated_date = updated_date[0]\n",
    "            if isinstance(expiration_date, list):\n",
    "                expiration_date = expiration_date[0]\n",
    "            \n",
    "            features['domain_age'] = (datetime.now() - creation_date).days if creation_date else None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching domain info for {domain}: {e}\")\n",
    "            features['domain_age'] = None\n",
    "\n",
    "        features['has_ssl'] = 1 if int(check_ssl(domain)) else 0\n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        features.update(features)\n",
    "    else:\n",
    "        features.update({\n",
    "            'html_length': 0,\n",
    "            'js_length': 0,\n",
    "            'num_links': 0,\n",
    "            'num_forms': 0,\n",
    "            'contains_ip': 0,\n",
    "            'shortening_services': 0,\n",
    "            'url_length': len(url),\n",
    "            'special_chars': sum(not c.isalnum() and c not in ['.', '-', '_'] for c in url),\n",
    "            'domain_age': None,\n",
    "            'has_ssl': 0,\n",
    "        })\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ee4367-8ba0-4ef8-a6fc-2c25846efcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract features running\n"
     ]
    }
   ],
   "source": [
    "                                              #3. Feature Vector\n",
    "print(\"Extract features running\")\n",
    "# Function to process a single URL\n",
    "def process_url(url):\n",
    "    try:\n",
    "        features = extract_features(url)\n",
    "        return {'url': url, **features}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL: {url}. Error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1b856c35-6024-459e-8738-249b61d56403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing running\n"
     ]
    }
   ],
   "source": [
    "                             #5. Data Preprocessing - Randomly process 200 URLs for each label\n",
    "import pandas as pd\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"Data preprocessing running\")\n",
    "def preprocess_data(df, processed_df, n=100):\n",
    "    # Separate URLs by label (0 for legitimate, 1 for phishing)\n",
    "    legitimate_urls = df[df['Label'] == 0]['url'].tolist()\n",
    "    phishing_urls = df[df['Label'] == 1]['url'].tolist()\n",
    "    \n",
    "    # Filter out URLs that have already been processed\n",
    "    already_processed_urls = set(processed_df['url'].tolist()) if not processed_df.empty else set()\n",
    "    legitimate_urls = [url for url in legitimate_urls if url not in already_processed_urls]\n",
    "    phishing_urls = [url for url in phishing_urls if url not in already_processed_urls]\n",
    "\n",
    "    # Randomly sample 1000 URLs from each group (or fewer if not enough remain)\n",
    "    legitimate_sample = random.sample(legitimate_urls, min(n, len(legitimate_urls)))\n",
    "    phishing_sample = random.sample(phishing_urls, min(n, len(phishing_urls)))\n",
    "\n",
    "    all_samples = legitimate_sample + phishing_sample\n",
    "    results = []\n",
    "    \n",
    "    # Multithreading to process URLs faster\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        futures = {executor.submit(process_url, url): url for url in all_samples}\n",
    "        for future in as_completed(futures):\n",
    "            url = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:  # Only append if the URL is successfully processed\n",
    "                    results.append(result)\n",
    "                    print(f\"Processed URL: {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL: {url}. Error: {e}\")\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    X = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure the corresponding labels match the processed URLs\n",
    "    y = df.loc[df['url'].isin(X['url']), 'Label'].values\n",
    "    print(X)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f16cf2-9c94-485f-8067-9295176ef308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Historical Dataset\n",
      "Processed URL: http://mashable.com\n",
      "Error fetching domain info for telegraf.com.ua: unsupported operand type(s) for -: 'datetime.datetime' and 'str'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 14:48:46,478 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed URL: http://elitedaily.com\n",
      "Processed URL: http://telegraf.com.ua\n",
      "Processed URL: http://twitter.com\n",
      "Processed URL: http://teespring.com\n",
      "Processed URL: http://patch.com\n",
      "Processed URL: http://manager.co.th\n",
      "Processed URL: http://metro.co.uk\n",
      "Processed URL: http://ameblo.jp\n",
      "Processed URL: http://myegy.to\n",
      "Error fetching domain info for mirtesen.ru: No entries found for the selected source(s).\n",
      "\n",
      ">>> Last update of WHOIS database: 2024.09.16T16:48:39Z <<<\n",
      "\n",
      "Processed URL: http://akhbarelyom.com\n",
      "Processed URL: http://motthegioi.vn\n",
      "Processed URL: http://europa.eu\n",
      "Processed URL: http://mainichi.jp\n",
      "Processed URL: http://askubuntu.com\n",
      "Processed URL: http://extratorrent.cc\n",
      "Processed URL: http://avxhome.se\n",
      "Processed URL: http://indianexpress.com\n",
      "Processed URL: http://livetv.sx\n",
      "Processed URL: http://nymag.com\n",
      "Processed URL: http://nguyentandung.org\n",
      "Processed URL: http://tobogo.net\n",
      "Processed URL: http://mirtesen.ru\n",
      "Processed URL: http://paytm.com\n",
      "Processed URL: http://doodle.com\n",
      "Processed URL: http://ecnavi.jp\n",
      "Processed URL: http://fishki.net\n",
      "Processed URL: http://likemag.com\n",
      "Processed URL: http://jezebel.com\n",
      "Processed URL: http://momoshop.com.tw\n",
      "Processed URL: http://yourlust.com\n",
      "Processed URL: http://bgr.com\n",
      "Processed URL: http://pchome.com.tw\n",
      "Processed URL: http://2gis.ru\n",
      "Processed URL: http://auto.ru\n",
      "Processed URL: http://syosetu.com\n",
      "Processed URL: http://gov.uk\n",
      "Processed URL: http://bleacherreport.com\n",
      "Processed URL: http://atwiki.jp\n",
      "Processed URL: http://stackexchange.com\n",
      "Processed URL: http://skyrock.com\n",
      "Processed URL: http://web.de\n",
      "Processed URL: http://foursquare.com\n",
      "Processed URL: http://hubpages.com\n",
      "Processed URL: http://cox.com\n",
      "Processed URL: http://grantland.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 14:48:53,264 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno 111] Connection refused\n",
      "2024-09-16 14:48:53,762 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed URL: http://fortune.com\n",
      "Processed URL: http://iconosquare.com\n",
      "Processed URL: http://xueqiu.com\n",
      "Processed URL: http://moviepilot.com\n",
      "Processed URL: http://mentalfloss.com\n",
      "Processed URL: http://kickass.to\n",
      "Processed URL: http://css-tricks.com\n",
      "Processed URL: http://katproxy.com\n",
      "Processed URL: http://pantip.com\n",
      "Processed URL: http://wikimapia.org\n",
      "Processed URL: http://subscene.com\n",
      "Error fetching domain info for seasonvar.ru: No entries found for the selected source(s).\n",
      "\n",
      ">>> Last update of WHOIS database: 2024.09.16T16:48:46Z <<<\n",
      "\n",
      "Processed URL: http://kenh14.vn\n",
      "Processed URL: http://filehippo.com\n",
      "Error fetching domain info for privatbank.ua: unsupported operand type(s) for -: 'datetime.datetime' and 'str'\n",
      "Processed URL: http://9gag.tv\n",
      "Processed URL: http://anysex.com\n",
      "Processed URL: http://bongda88.info\n",
      "Processed URL: http://tsite.jp\n",
      "Processed URL: http://stackoverflow.com\n",
      "Processed URL: http://mediaset.it\n",
      "Processed URL: http://shareba.com\n",
      "Processed URL: http://myspace.com\n",
      "Processed URL: http://privatbank.ua\n",
      "Processed URL: http://maybank2u.com.my\n",
      "Processed URL: http://taboola.com\n",
      "Processed URL: http://kizi.com\n",
      "Processed URL: http://nhs.uk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 14:48:57,298 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed URL: http://xvideo-jp.com\n",
      "Processed URL: http://bluegape.com\n",
      "Processed URL: http://sourceforge.net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 14:48:57,598 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching domain info for proxyx.ru: No entries found for the selected source(s).\n",
      "\n",
      ">>> Last update of WHOIS database: 2024.09.16T16:48:48Z <<<\n",
      "\n",
      "Processed URL: http://bdnews24.com\n",
      "Processed URL: http://bitbucket.org\n",
      "Processed URL: http://clien.net\n",
      "Processed URL: http://amoory.com\n",
      "Processed URL: http://themeforest.net\n",
      "Processed URL: http://fanpage.gr\n",
      "Processed URL: http://olx.pl\n",
      "Processed URL: http://kienthuc.net.vn\n",
      "Processed URL: http://proxyx.ru\n",
      "Processed URL: http://dealnews.com\n",
      "Processed URL: http://xhamster.com\n",
      "Processed URL: http://truckcalling.com\n",
      "Processed URL: http://khabaronline.ir\n",
      "Processed URL: http://kakaku.com\n",
      "Error fetching domain info for serverinfo1policy.blogspot.com: No match for \"SERVERINFO1POLICY.BLOGSPOT.COM\".\n",
      ">>> Last update of whois database: 2024-09-16T13:48:39Z <<<\n",
      "\n",
      "NOTICE: The expiration date displayed in this record is the date the\n",
      "registrar's sponsorship of the domain name registration in the registry is\n",
      "currently set to expire. This date does not necessarily reflect the expiration\n",
      "date of the domain name registrant's agreement with the sponsoring\n",
      "registrar.  Users may consult the sponsoring registrar's Whois database to\n",
      "view the registrar's reported date of expiration for this registration.\n",
      "\n",
      "TERMS OF USE: You are not authorized to access or query our Whois\n",
      "database through the use of electronic processes that are high-volume and\n",
      "automated except as reasonably necessary to register domain names or\n",
      "modify existing registrations; the Data in VeriSign Global Registry\n",
      "Services' (\"VeriSign\") Whois database is provided by VeriSign for\n",
      "information purposes only, and to assist persons in obtaining information\n",
      "about or related to a domain name registration record. VeriSign does not\n",
      "guarantee its accuracy. By submitting a Whois query, you agree to abide\n",
      "by the following terms of use: You agree that you may use this Data only\n",
      "for lawful purposes and that under no circumstances will you use this Data\n",
      "to: (1) allow, enable, or otherwise support the transmission of mass\n",
      "unsolicited, commercial advertising or solicitations via e-mail, telephone,\n",
      "or facsimile; or (2) enable high volume, automated, electronic processes\n",
      "that apply to VeriSign (or its computer systems). The compilation,\n",
      "repackaging, dissemination or other use of this Data is expressly\n",
      "prohibited without the prior written consent of VeriSign. You agree not to\n",
      "use electronic processes that are automated and high-volume to access or\n",
      "query the Whois database except as reasonably necessary to register\n",
      "domain names or modify existing registrations. VeriSign reserves the right\n",
      "to restrict your access to the Whois database in its sole discretion to ensure\n",
      "operational stability.  VeriSign may restrict or terminate your access to the\n",
      "Whois database for failure to abide by these terms of use. VeriSign\n",
      "reserves the right to modify these terms at any time.\n",
      "\n",
      "The Registry database contains ONLY .COM, .NET, .EDU domains and\n",
      "Registrars.\n",
      "\n",
      "Processed URL: http://serverinfo1policy.blogspot.com\n",
      "Processed URL: http://smallseotools.com\n",
      "Processed URL: http://caviarkelp.com\n",
      "Processed URL: http://seasonvar.ru\n",
      "Processed URL: http://adelahidalgo.com\n",
      "Processed URL: http://0nexdrlvew.best\n",
      "Processed URL: http://stpetersquare.net\n",
      "Processed URL: http://spankbang.com\n",
      "Processed URL: http://rohini-marriageworld.com\n",
      "Processed URL: http://photos-restoration.com\n",
      "Processed URL: http://progarchives.com\n",
      "Processed URL: http://u4131462.ct.sendgrid.net\n",
      "Processed URL: http://nhms.solutions\n",
      "Processed URL: http://portraitomar.net\n",
      "Processed URL: http://utffunds.com\n",
      "Processed URL: http://alfalfalfa.com\n",
      "Processed URL: http://restaurantelabarquita.com\n",
      "Processed URL: http://webkc.dede.go.th\n",
      "Processed URL: http://dropbox-com.dropbox.lilyskitchen.skyfencenet.com\n",
      "Processed URL: http://kanakapolymers.com\n",
      "Processed URL: http://eventdofus.com\n",
      "Processed URL: http://flavena.co.rs\n",
      "Processed URL: http://betasus21.blogspot.com\n",
      "Processed URL: http://belshop.fitcurves.org\n",
      "Processed URL: http://sgozq-alternate.app.link\n",
      "Processed URL: http://allrecipes.com\n",
      "Processed URL: http://sardineroabogados.com\n",
      "Processed URL: http://digg.com\n",
      "Processed URL: http://koskas.activehosted.com\n",
      "Processed URL: http://nwkc.adventist.or.ke\n",
      "Processed URL: http://cimer-basvurcomtr.com\n",
      "Processed URL: http://cns-international2.com\n",
      "Processed URL: http://paypal-helps-seller-ebay.gitlab.io\n",
      "Processed URL: http://technoroute53.xyz\n",
      "Processed URL: http://rmic.simply-winspace.it\n",
      "Processed URL: http://google.com.mx\n",
      "Processed URL: http://rockysite.net\n",
      "Processed URL: http://mandan.com.tr\n",
      "Processed URL: http://docerom.ro\n",
      "Processed URL: http://d2ob6ssuz1ej59.cloudfront.net\n",
      "Processed URL: http://protourssoccer.com\n",
      "Processed URL: http://challengedcreditautos.com\n",
      "Processed URL: http://estilocouros.com.br\n",
      "Processed URL: http://chat-whatsapp1.grup-wa69.ml\n",
      "Processed URL: http://ykm.de\n",
      "Processed URL: http://spaziozero.org\n",
      "Processed URL: http://visteme.mx\n",
      "Processed URL: http://w2-bb-acesso-online.cloud-fr1.unispace.io\n",
      "Processed URL: http://poddar-wonder-city.co.in\n",
      "Processed URL: http://admin97759.typeform.com\n",
      "Processed URL: http://vksafe.com\n",
      "Processed URL: http://amazon.de.signin.verification.openid.5935156.conleyfaili.tk\n",
      "Processed URL: http://barberiamartinz.com\n",
      "Processed URL: http://dropbox.com\n",
      "Processed URL: http://newsletter9707fb85e61e055593f03a43ab.dns-cloud.net\n",
      "Processed URL: http://betasusgirisadresi.blogspot.com\n",
      "Processed URL: http://sobhainternationalcitygurgaon.in\n",
      "Processed URL: http://3xportfolio.com\n",
      "Processed URL: http://fareast.qa\n",
      "Processed URL: http://tubepchiunuoc.com\n",
      "Processed URL: http://adventures9.com\n",
      "Processed URL: http://pmasc.co.uk\n",
      "Processed URL: http://adleer-ad.com\n",
      "Processed URL: http://tnlawexpress.com\n",
      "Processed URL: http://fb-recovery-10000076859-it.tk\n",
      "Processed URL: http://staytechconsulting.com\n",
      "Processed URL: http://wuarface.ru:443\n",
      "Processed URL: http://safgg34rfdfweqdas.epizy.com\n",
      "Processed URL: http://stevencrews.com\n",
      "Processed URL: http://cap114.fr\n",
      "Processed URL: http://xtremefish.su\n",
      "Processed URL: http://shafeimall046.top\n",
      "Processed URL: http://ihaleile.com\n",
      "Processed URL: http://restaurantecuevaspalacios.com\n",
      "Processed URL: http://tuscaloosanephrology.com\n",
      "Processed URL: http://work.jonwillchambers.co.uk\n",
      "Processed URL: http://expressshippers.net\n",
      "Processed URL: http://mail.oficinas-valencia.com\n",
      "Processed URL: http://barclaycard.signln.link\n",
      "Processed URL: http://iptf.ir\n",
      "Processed URL: http://resgatemobilebb.com\n",
      "Processed URL: http://gulhasal.com\n",
      "Processed URL: http://secure.runescape.com-un.ru\n",
      "Processed URL: http://keepsolution.com\n",
      "Processed URL: http://suumo.jp\n",
      "Processed URL: http://treasurechicks.com\n",
      "Processed URL: http://couttstrustcreditunion.co.uk\n",
      "Processed URL: http://unity3d.com\n",
      "Processed URL: http://webmailadmin0.myfreesites.net\n",
      "Processed URL: http://tam-multiplosresgate.com\n",
      "Processed URL: http://bluereparation.com\n",
      "Processed URL: http://hm.rc-verify5.com\n",
      "Processed URL: http://ip-132-148-145-99.ip.secureserver.net\n",
      "Processed URL: http://rapline.hu\n",
      "Processed URL: http://megamagaluon.com\n",
      "Processed URL: http://outridebd.com\n",
      "Processed URL: http://r67987yq.beget.tech\n",
      "Processed URL: http://upd-medlav.net\n",
      "Processed URL: http://b9dm.com\n",
      "Processed URL: http://illiberal-session.000webhostapp.com\n",
      "Processed URL: http://secure.counterpath.com\n",
      "Processed URL: http://irs.org\n",
      "Processed URL: http://ringring.vn\n",
      "Processed URL: http://correios.com.br\n",
      "Processed URL: http://zozo.jp\n",
      "Processed URL: http://78.143.96.35\n",
      "Processed URL: http://mail.speedy.com.pk\n",
      "Processed URL: http://grupdewasa18.whatsappp.my.id\n",
      "Processed URL: http://u1035597.cp.regruhosting.ru\n",
      "Processed URL: http://soundcloud.com\n",
      "Processed URL: http://cobrancanet.com.br\n",
      "Feature matrix shape: (200, 11)\n",
      "Labels shape: 200\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Historical Dataset\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv('legitimate_urls.csv')\n",
    "df2 = pd.read_csv('phishing_urls.csv')\n",
    "\n",
    "# \"legitimate\" is identified as  0 in df1 and\"phishing\" is identified as 1 in df2\n",
    "\n",
    "# Concatenate the datasets\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "#add http:// to the domain and create a column url\n",
    "# Prepend 'http://' to the 'Domain' column\n",
    "df_combined['url'] = df_combined['Domain'].apply(lambda x: f'http://{x}' if not x.startswith('https://') else x)\n",
    "\n",
    "#remove duplicates\n",
    "df = df_combined.drop_duplicates(subset='url')  # Remove duplicates\n",
    "# Assuming we have a dataframe 'processed_df' of already processed URLs, or create an empty one\n",
    "\n",
    "processed_df = pd.DataFrame(columns=['url'])\n",
    "\n",
    "# Preprocess the data (sample 1000 legitimate and 200 phishing URLs for feature extraction)\n",
    "X, y = preprocess_data(df, processed_df, n=100)\n",
    "\n",
    "# At this point, `X` contains the feature set and `y` contains the corresponding labels for training\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {len(y)}\")\n",
    "# X and y can now be used for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9f5f5337-b981-4891-bbc8-f41d7b95cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Data Preprocessing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_length</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>shortening_services</th>\n",
       "      <th>special_chars</th>\n",
       "      <th>html_length</th>\n",
       "      <th>js_length</th>\n",
       "      <th>num_links</th>\n",
       "      <th>num_forms</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>has_ssl</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215572</td>\n",
       "      <td>68466</td>\n",
       "      <td>359</td>\n",
       "      <td>1</td>\n",
       "      <td>6998.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339507</td>\n",
       "      <td>53337</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>4765.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2610</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9004.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43273</td>\n",
       "      <td>20758</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4967.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>302163</td>\n",
       "      <td>131333</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>11157.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_length  contains_ip  shortening_services  special_chars  html_length  \\\n",
       "0          19            0                    0              0       215572   \n",
       "1          21            0                    0              0       339507   \n",
       "3          18            0                    0              0         2610   \n",
       "4          20            0                    0              0        43273   \n",
       "5          16            0                    0              0       302163   \n",
       "\n",
       "   js_length  num_links  num_forms  domain_age  has_ssl  Label  \n",
       "0      68466        359          1      6998.0        1      0  \n",
       "1      53337        158          3      4765.0        1      0  \n",
       "3        136          0          0      9004.0        1      0  \n",
       "4      20758         55          0      4967.0        1      0  \n",
       "5     131333        216          1     11157.0        1      0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                       #5. Data Preprocessing\n",
    "print(\"Running Data Preprocessing\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocess features\n",
    "processed_df = pd.DataFrame(X)\n",
    "processed_df['Label']= y\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "83833e4c-b5a1-4b44-a1dd-08f8c293daa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_length             0\n",
      "contains_ip            0\n",
      "shortening_services    0\n",
      "special_chars          0\n",
      "html_length            0\n",
      "js_length              0\n",
      "num_links              0\n",
      "num_forms              0\n",
      "domain_age             0\n",
      "has_ssl                0\n",
      "Label                  0\n",
      "dtype: int64\n",
      "Remaining NaN values: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_length</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>shortening_services</th>\n",
       "      <th>special_chars</th>\n",
       "      <th>html_length</th>\n",
       "      <th>js_length</th>\n",
       "      <th>num_links</th>\n",
       "      <th>num_forms</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>has_ssl</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215572</td>\n",
       "      <td>68466</td>\n",
       "      <td>359</td>\n",
       "      <td>1</td>\n",
       "      <td>6998.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339507</td>\n",
       "      <td>53337</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>4765.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2610</td>\n",
       "      <td>136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9004.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43273</td>\n",
       "      <td>20758</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>4967.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>302163</td>\n",
       "      <td>131333</td>\n",
       "      <td>216</td>\n",
       "      <td>1</td>\n",
       "      <td>11157.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   url_length  contains_ip  shortening_services  special_chars  html_length  \\\n",
       "0          19            0                    0              0       215572   \n",
       "1          21            0                    0              0       339507   \n",
       "3          18            0                    0              0         2610   \n",
       "4          20            0                    0              0        43273   \n",
       "5          16            0                    0              0       302163   \n",
       "\n",
       "   js_length  num_links  num_forms  domain_age  has_ssl  Label  \n",
       "0      68466        359          1      6998.0        1      0  \n",
       "1      53337        158          3      4765.0        1      0  \n",
       "3        136          0          0      9004.0        1      0  \n",
       "4      20758         55          0      4967.0        1      0  \n",
       "5     131333        216          1     11157.0        1      0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                        #5. Data Preprocessing\n",
    "# Remove rows with any NaN values in the dataframe\n",
    "processed_df = processed_df.dropna()\n",
    "\n",
    "# Check if there are any NaN values left (should print 0 for all columns)\n",
    "print(processed_df.isnull().sum())\n",
    "\n",
    "# Verify if any NaN values remain\n",
    "print(f\"Remaining NaN values: {processed_df.isna().sum().sum()}\")  # This should print 0 if no NaNs are left\n",
    "#save to csv file\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3efc0b0-53a1-436d-944c-dd0e4b195393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Seperate the legitimate represented as 0 and phishing representd as 1 dataframes\\nlegit_df = processed_df[processed_df['Label'] == 0].reset_index(drop=True)  # Rows with status == 0\\nphish_df = processed_df[processed_df['status'] == 1].reset_index(drop=True)    # Rows with status == 1\\n#concat the legitimate and phishing\\n# Concatenate legitimate_df and phishing_df\\nprocessed_df = pd.concat([legit_df, phish_df], ignore_index=True)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                              #5. Data Preprocessing\n",
    "'''\n",
    "#Seperate the legitimate represented as 0 and phishing representd as 1 dataframes\n",
    "legit_df = processed_df[processed_df['Label'] == 0].reset_index(drop=True)  # Rows with status == 0\n",
    "phish_df = processed_df[processed_df['status'] == 1].reset_index(drop=True)    # Rows with status == 1\n",
    "#concat the legitimate and phishing\n",
    "# Concatenate legitimate_df and phishing_df\n",
    "processed_df = pd.concat([legit_df, phish_df], ignore_index=True)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d2900633-0aad-4b5a-9976-5316cec94096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url and Label have been dropped and not found in the dataframe\n"
     ]
    }
   ],
   "source": [
    "                              #6-7. Feature Scaling and Feature Selection\n",
    "\n",
    "try:\n",
    "    # Split data\n",
    "    X = processed_df.drop(columns=['url', 'Label'])\n",
    "    y = processed_df['Label']\n",
    "except: \n",
    "    print('url and Label have been dropped and not found in the dataframe')\n",
    "finally:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b19ab6da-2bb7-40bb-bdc9-798102a0415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "                #8. Machine Learning Models such as RandomForest, Support Vector, and GradentBoosting\n",
    "\n",
    "#Define classifiers\n",
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'GradientBoosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9cc9ec5-c0a9-44d7-a245-87e2878f5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            #9. Hyperparameter Tuning\n",
    "# Define parameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f677d4c-be99-4b07-ac11-504a1648451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForest...\n",
      "RandomForest - Mean Cross-Validation Accuracy: 0.6487\n",
      "RandomForest - Accuracy: 0.8125, Precision: 0.5, Recall: 0.6666666666666666, F1 Score: 0.5714285714285714\n",
      "Training SVM...\n",
      "SVM - Mean Cross-Validation Accuracy: 0.7295\n",
      "SVM - Accuracy: 0.875, Precision: 1.0, Recall: 0.3333333333333333, F1 Score: 0.5\n",
      "Training GradientBoosting...\n",
      "GradientBoosting - Mean Cross-Validation Accuracy: 0.6795\n",
      "GradientBoosting - Accuracy: 0.8125, Precision: 0.5, Recall: 0.3333333333333333, F1 Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "                              #10. Model selection, training and evaluation metrics\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "# Model selection, training and evaluation\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    mean_cv_score = cv_scores.mean()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "     # Evaluate on test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')  # Use 'binary' for binary classification\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "    print(f\"{name} - Mean Cross-Validation Accuracy: {mean_cv_score:.4f}\") \n",
    "    print(f\"{name} - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "    \n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2f02cc9-15d1-46a4-b289-6592a0deb0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: SVC(C=0.1, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "                                       #11. Final Model Selection\n",
    "print(f\"Best model: {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6f7494b9-5910-4796-af22-31859d359f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                       #12. Classification\n",
    "def classify_url(url, model, scaler, feature_columns_order):\n",
    "    features = extract_features(url)\n",
    "    features_df = pd.DataFrame([features])\n",
    "    # Ensure all required columns are present\n",
    "    for col in feature_columns_order:\n",
    "        if col not in features_df.columns:\n",
    "            features_df[col] = 0\n",
    "    \n",
    "    features_df = features_df[feature_columns_order]\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    features_df = features_df.fillna(0)\n",
    "    \n",
    "    # Scale and predict\n",
    "    features_scaled = scaler.transform(features_df)\n",
    "    prediction = model.predict(features_scaled)\n",
    "    print(prediction)\n",
    "    return \"Legitimate\" if prediction == 0 else \"Phishing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3df4629b-fb32-407a-a701-1c1c0d537d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter new url\n",
    "new_url = 'http://345666'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "432e49c2-3497-45af-9618-ba07de0e3fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "The URL 'http://345666' is classified as: Phishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56796/657799181.py:13: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  features_df = features_df.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "classification_result = classify_url(new_url, best_model, scaler, X.columns)\n",
    "print(f\"The URL '{new_url}' is classified as: {classification_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b63e49f2-5d7d-4c1a-ae4f-ff3e467c8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the model to use to create a web interface\n",
    "import pickle\n",
    "\n",
    "# Save the model\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(best_model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0dfe3e57-da3c-4569-9857-68bd0c0ce27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f9f4d750-9fa1-40cd-a392-5e1b66e415ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5699fdf-49c1-4106-9912-94696fe02086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
